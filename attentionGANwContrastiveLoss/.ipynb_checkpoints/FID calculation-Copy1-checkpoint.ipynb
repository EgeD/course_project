{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26cf0bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import cv2\n",
    "import os\n",
    "import random\n",
    "import wandb\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "from contrastive_attention import ContrastiveAttention\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from visualizer import Visualizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4c1c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor2im(input_image, imtype=np.uint8):\n",
    "    \"\"\"\"Converts a Tensor array into a numpy image array.\n",
    "\n",
    "    Parameters:\n",
    "        input_image (tensor) --  the input image tensor array\n",
    "        imtype (type)        --  the desired type of the converted numpy array\n",
    "    \"\"\"\n",
    "    if not isinstance(input_image, np.ndarray):\n",
    "        if isinstance(input_image, torch.Tensor):  # get the data from a variable\n",
    "            image_tensor = input_image.data\n",
    "        else:\n",
    "            return input_image\n",
    "        image_numpy = image_tensor[0].clamp(-1.0, 1.0).cpu().float().numpy()  # convert it into a numpy array\n",
    "        if image_numpy.shape[0] == 1:  # grayscale to RGB\n",
    "            image_numpy = np.tile(image_numpy, (3, 1, 1))\n",
    "        image_numpy = (np.transpose(image_numpy, (1, 2, 0)) + 1) / 2.0 * 255.0  # post-processing: tranpose and scaling\n",
    "    else:  # if it is a numpy array, do nothing\n",
    "        image_numpy = input_image\n",
    "    return image_numpy.astype(imtype)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69228f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.ToTensor()\n",
    "\n",
    "my_model_checkpoint ='/kuacc/users/edincer16/Comp541_fall22/course_project/attentionGANwContrastiveLoss/model_results/ContrastiveAttention_weights/ContrastiveAttention_400_net_G.pth'   \n",
    "my_model = ContrastiveAttention(input_dim=3,output_dim=3,n_epochs=5).cuda()\n",
    "my_model.netG.load_state_dict(torch.load(my_model_checkpoint))\n",
    "my_model.netG.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3ece7d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 120/120 [00:10<00:00, 11.86it/s]\n"
     ]
    }
   ],
   "source": [
    "input_path = '/kuacc/users/edincer16/Comp541_fall22/course_project/attentionGAN/horse2zebra/testA/*'\n",
    "output_path = '/kuacc/users/edincer16/Comp541_fall22/course_project/attentionGAN/model_results/baseline_outputs_for_fid/'\n",
    "for img_path in tqdm(glob(input_path)):\n",
    "    img_name = img_path.split('/')[-1]\n",
    "    img_init = cv2.imread(img_path)\n",
    "    img = cv2.cvtColor(img_init, cv2.COLOR_BGR2RGB)\n",
    "    img = transform(img).unsqueeze(0)\n",
    "    my_generated_image, _, _, _, _, _, _, _, _, _, _, \\\n",
    "            _, _, _, _, _, _, _, _, _, _, \\\n",
    "            _, _, _, _, _, _, _, _, _  = my_model.netG_A(img.cuda())\n",
    "    gen_img = tensor2im(my_generated_image)\n",
    "    cv2.imwrite(output_path+img_name,\n",
    "                cv2.cvtColor(gen_img, cv2.COLOR_RGB2BGR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed606378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 3/3 [00:01<00:00,  1.83it/s]\n",
      "100%|█████████████████████████████████████████████| 3/3 [00:01<00:00,  2.05it/s]\n",
      "FID:  224.84613974186607\n"
     ]
    }
   ],
   "source": [
    "#FID calculation\n",
    "!python -m pytorch_fid /kuacc/users/edincer16/Comp541_fall22/course_project/attentionGAN/horse2zebra/testA /kuacc/users/edincer16/Comp541_fall22/course_project/attentionGAN/model_results/baseline_outputs_for_fid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b50c03b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
