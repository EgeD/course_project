Activating Python 3.6.3...
Setting stack size to unlimited...
core file size          (blocks, -c) 0
data seg size           (kbytes, -d) unlimited
scheduling priority             (-e) 0
file size               (blocks, -f) unlimited
pending signals                 (-i) 2061310
max locked memory       (kbytes, -l) unlimited
max memory size         (kbytes, -m) 33554432
open files                      (-n) 51200
pipe size            (512 bytes, -p) 8
POSIX message queues     (bytes, -q) 819200
real-time priority              (-r) 0
stack size              (kbytes, -s) unlimited
cpu time               (seconds, -t) unlimited
max user processes              (-u) 4096
virtual memory          (kbytes, -v) unlimited
file locks                      (-x) unlimited

/kuacc/users/edincer16/.conda/envs/cyclegan/lib/python3.8/site-packages/torchvision/transforms/transforms.py:287: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
wandb: Currently logged in as: edincer16 (use `wandb login --relogin` to force relogin)
wandb: wandb version 0.13.6 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.9
wandb: Syncing run AttentionGAN_baseline
wandb:  View project at https://wandb.ai/edincer16/CycleGAN-SlotAttention
wandb:  View run at https://wandb.ai/edincer16/CycleGAN-SlotAttention/runs/ion8j27r
wandb: Run data is saved locally in /scratch/users/edincer16/Comp541_fall22/course_project/attentionGAN/wandb/run-20221214_022456-ion8j27r
wandb: Run `wandb offline` to turn off syncing.

The number of training images = 1334
  0% 0/401 [00:00<?, ?it/s]/kuacc/users/edincer16/.conda/envs/cyclegan/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:129: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
  0% 0/401 [15:56<?, ?it/s]
learning rate 0.0002000 -> 0.0002000
Traceback (most recent call last):
  File "train.py", line 162, in <module>
    image = transform(img_tmp).unsqueeze(0)
  File "/kuacc/users/edincer16/.conda/envs/cyclegan/lib/python3.8/site-packages/torchvision/transforms/transforms.py", line 61, in __call__
    img = t(img)
  File "/kuacc/users/edincer16/.conda/envs/cyclegan/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/kuacc/users/edincer16/.conda/envs/cyclegan/lib/python3.8/site-packages/torchvision/transforms/transforms.py", line 304, in forward
    return F.resize(img, self.size, self.interpolation, self.max_size, self.antialias)
  File "/kuacc/users/edincer16/.conda/envs/cyclegan/lib/python3.8/site-packages/torchvision/transforms/functional.py", line 419, in resize
    return F_pil.resize(img, size=size, interpolation=pil_interpolation, max_size=max_size)
  File "/kuacc/users/edincer16/.conda/envs/cyclegan/lib/python3.8/site-packages/torchvision/transforms/functional_pil.py", line 233, in resize
    raise TypeError('img should be PIL Image. Got {}'.format(type(img)))
TypeError: img should be PIL Image. Got <class 'numpy.ndarray'>
wandb: Waiting for W&B process to finish, PID 112523... (failed 1). Press ctrl-c to abort syncing.
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: | 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: / 0.00MB of 0.01MB uploaded (0.00MB deduped)wandb: - 0.00MB of 0.01MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.01MB uploaded (0.00MB deduped)wandb: | 0.00MB of 0.01MB uploaded (0.00MB deduped)wandb: / 0.00MB of 0.01MB uploaded (0.00MB deduped)wandb: - 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: \ 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: | 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: / 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: - 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: \ 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Synced AttentionGAN_baseline: https://wandb.ai/edincer16/CycleGAN-SlotAttention/runs/ion8j27r
wandb: Find logs at: ./wandb/run-20221214_022456-ion8j27r/logs/debug.log
wandb: 

