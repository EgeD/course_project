Activating Python 3.6.3...
Setting stack size to unlimited...
core file size          (blocks, -c) 0
data seg size           (kbytes, -d) unlimited
scheduling priority             (-e) 0
file size               (blocks, -f) unlimited
pending signals                 (-i) 2061323
max locked memory       (kbytes, -l) unlimited
max memory size         (kbytes, -m) 33554432
open files                      (-n) 51200
pipe size            (512 bytes, -p) 8
POSIX message queues     (bytes, -q) 819200
real-time priority              (-r) 0
stack size              (kbytes, -s) unlimited
cpu time               (seconds, -t) unlimited
max user processes              (-u) 4096
virtual memory          (kbytes, -v) unlimited
file locks                      (-x) unlimited

/kuacc/users/edincer16/.conda/envs/cyclegan/lib/python3.8/site-packages/torchvision/transforms/transforms.py:287: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
wandb: Currently logged in as: edincer16 (use `wandb login --relogin` to force relogin)
wandb: wandb version 0.13.6 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.9
wandb: Syncing run AttentionGAN_baseline
wandb:  View project at https://wandb.ai/edincer16/CycleGAN-SlotAttention
wandb:  View run at https://wandb.ai/edincer16/CycleGAN-SlotAttention/runs/wqa8jv7c
wandb: Run data is saved locally in /scratch/users/edincer16/Comp541_fall22/course_project/attentionGAN/wandb/run-20221214_093629-wqa8jv7c
wandb: Run `wandb offline` to turn off syncing.
  0% 0/401 [00:00<?, ?it/s]
The number of training images = 1334
/kuacc/users/edincer16/.conda/envs/cyclegan/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:129: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
  0% 0/401 [15:32<?, ?it/s]learning rate 0.0002000 -> 0.0002000

Traceback (most recent call last):
  File "train.py", line 164, in <module>
    generated_image,_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, \
ValueError: not enough values to unpack (expected 60, got 30)
wandb: Waiting for W&B process to finish, PID 111496... (failed 1). Press ctrl-c to abort syncing.
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: | 0.00MB of 0.01MB uploaded (0.00MB deduped)wandb: / 0.00MB of 0.01MB uploaded (0.00MB deduped)wandb: - 0.00MB of 0.01MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.01MB uploaded (0.00MB deduped)wandb: | 0.00MB of 0.01MB uploaded (0.00MB deduped)wandb: / 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: - 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: \ 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: | 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: / 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: - 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: \ 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Synced AttentionGAN_baseline: https://wandb.ai/edincer16/CycleGAN-SlotAttention/runs/wqa8jv7c
wandb: Find logs at: ./wandb/run-20221214_093629-wqa8jv7c/logs/debug.log
wandb: 

